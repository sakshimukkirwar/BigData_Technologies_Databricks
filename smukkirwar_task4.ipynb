{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e1e2579-c923-4a63-9ef7-1d486898df52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=3045796155885445#setting/sparkui/1103-195544-dpu1e75b/driver-744248655240730315\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=3045796155885445#setting/sparkui/1103-195544-dpu1e75b/driver-744248655240730315\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4b9c2a2-d791-4129-a9d9-52a48e2b9f71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting google-cloud-automl\n  Downloading google_cloud_automl-2.11.3-py2.py3-none-any.whl (328 kB)\nCollecting google-cloud-storage\n  Downloading google_cloud_storage-2.13.0-py2.py3-none-any.whl (121 kB)\nCollecting google-cloud-bigquery==2.3.1\n  Downloading google_cloud_bigquery-2.3.1-py2.py3-none-any.whl (208 kB)\nCollecting google-cloud-texttospeech\n  Downloading google_cloud_texttospeech-2.14.2-py2.py3-none-any.whl (138 kB)\nCollecting google-cloud-speech\n  Downloading google_cloud_speech-2.22.0-py2.py3-none-any.whl (275 kB)\nCollecting google-api-core[grpc]<2.0.0dev,>=1.23.0\n  Downloading google_api_core-1.34.0-py3-none-any.whl (120 kB)\nCollecting google-resumable-media<2.0dev,>=0.6.0\n  Downloading google_resumable_media-1.3.3-py2.py3-none-any.whl (75 kB)\nRequirement already satisfied: protobuf>=3.12.0 in /databricks/python3/lib/python3.9/site-packages (from google-cloud-bigquery==2.3.1) (3.19.4)\nCollecting google-cloud-core<2.0dev,>=1.4.1\n  Downloading google_cloud_core-1.7.3-py2.py3-none-any.whl (28 kB)\nCollecting proto-plus>=1.10.0\n  Downloading proto_plus-1.22.3-py3-none-any.whl (48 kB)\nRequirement already satisfied: six<2.0.0dev,>=1.13.0 in /databricks/python3/lib/python3.9/site-packages (from google-cloud-bigquery==2.3.1) (1.16.0)\nCollecting protobuf>=3.12.0\n  Downloading protobuf-4.25.0-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\nCollecting google-cloud-storage\n  Downloading google_cloud_storage-2.12.0-py2.py3-none-any.whl (120 kB)\n  Downloading google_cloud_storage-2.11.0-py2.py3-none-any.whl (118 kB)\n  Downloading google_cloud_storage-2.10.0-py2.py3-none-any.whl (114 kB)\nCollecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n  Downloading google_api_core-2.12.0-py3-none-any.whl (121 kB)\nCollecting google-auth<3.0dev,>=1.25.0\n  Downloading google_auth-2.23.4-py2.py3-none-any.whl (183 kB)\nCollecting google-cloud-storage\n  Downloading google_cloud_storage-2.9.0-py2.py3-none-any.whl (113 kB)\n  Downloading google_cloud_storage-2.8.0-py2.py3-none-any.whl (113 kB)\n  Downloading google_cloud_storage-2.7.0-py2.py3-none-any.whl (110 kB)\n  Downloading google_cloud_storage-2.6.0-py2.py3-none-any.whl (105 kB)\n  Downloading google_cloud_storage-2.5.0-py2.py3-none-any.whl (106 kB)\n  Downloading google_cloud_storage-2.4.0-py2.py3-none-any.whl (106 kB)\n  Downloading google_cloud_storage-2.3.0-py2.py3-none-any.whl (107 kB)\n  Downloading google_cloud_storage-2.2.1-py2.py3-none-any.whl (107 kB)\nRequirement already satisfied: requests<3.0.0dev,>=2.18.0 in /databricks/python3/lib/python3.9/site-packages (from google-cloud-storage) (2.27.1)\n  Downloading google_cloud_storage-2.2.0-py2.py3-none-any.whl (107 kB)\n  Downloading google_cloud_storage-2.1.0-py2.py3-none-any.whl (106 kB)\nCollecting googleapis-common-protos<2.0.dev0,>=1.56.2\n  Downloading googleapis_common_protos-1.61.0-py2.py3-none-any.whl (230 kB)\nCollecting protobuf>=3.12.0\n  Downloading protobuf-3.20.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\nCollecting grpcio-status<2.0dev,>=1.33.2\n  Downloading grpcio_status-1.59.2-py3-none-any.whl (14 kB)\nCollecting grpcio<2.0dev,>=1.33.2\n  Downloading grpcio-1.59.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\nCollecting rsa<5,>=3.1.4\n  Downloading rsa-4.9-py3-none-any.whl (34 kB)\nCollecting cachetools<6.0,>=2.0.0\n  Downloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)\nCollecting pyasn1-modules>=0.2.1\n  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\nCollecting google-cloud-core<2.0dev,>=1.4.1\n  Downloading google_cloud_core-1.7.2-py2.py3-none-any.whl (28 kB)\n  Downloading google_cloud_core-1.7.1-py2.py3-none-any.whl (28 kB)\n  Downloading google_cloud_core-1.7.0-py2.py3-none-any.whl (28 kB)\n  Downloading google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB)\nINFO: pip is looking at multiple versions of cachetools to determine which version is compatible with other requirements. This could take a while.\nCollecting cachetools<6.0,>=2.0.0\n  Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n  Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n  Downloading cachetools-5.2.1-py3-none-any.whl (9.3 kB)\n  Downloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n  Downloading cachetools-5.1.0-py3-none-any.whl (9.2 kB)\n  Downloading cachetools-5.0.0-py3-none-any.whl (9.1 kB)\n  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\nINFO: pip is looking at multiple versions of cachetools to determine which version is compatible with other requirements. This could take a while.\n  Downloading cachetools-4.2.2-py3-none-any.whl (11 kB)\n  Downloading cachetools-4.2.1-py3-none-any.whl (12 kB)\n  Downloading cachetools-4.2.0-py3-none-any.whl (12 kB)\n  Downloading cachetools-4.1.1-py3-none-any.whl (10 kB)\n  Downloading cachetools-4.1.0-py3-none-any.whl (10 kB)\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n  Downloading cachetools-4.0.0-py3-none-any.whl (10 kB)\n  Downloading cachetools-3.1.1-py2.py3-none-any.whl (11 kB)\n  Downloading cachetools-3.1.0-py2.py3-none-any.whl (12 kB)\n  Downloading cachetools-3.0.0-py2.py3-none-any.whl (12 kB)\n  Downloading cachetools-2.1.0-py2.py3-none-any.whl (12 kB)\n  Downloading cachetools-2.0.1-py2.py3-none-any.whl (11 kB)\n  Downloading cachetools-2.0.0-py2.py3-none-any.whl (11 kB)\nINFO: pip is looking at multiple versions of google-auth to determine which version is compatible with other requirements. This could take a while.\nCollecting google-auth<3.0dev,>=1.25.0\n  Downloading google_auth-2.23.3-py2.py3-none-any.whl (182 kB)\n  Downloading google_auth-2.23.2-py2.py3-none-any.whl (181 kB)\n  Downloading google_auth-2.23.1-py2.py3-none-any.whl (181 kB)\nCollecting urllib3>=2.0.5\n  Downloading urllib3-2.0.7-py3-none-any.whl (124 kB)\nCollecting google-auth<3.0dev,>=1.25.0\n  Downloading google_auth-2.23.0-py2.py3-none-any.whl (181 kB)\nRequirement already satisfied: urllib3<2.0 in /databricks/python3/lib/python3.9/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (1.26.9)\n  Downloading google_auth-2.22.0-py2.py3-none-any.whl (181 kB)\n  Downloading google_auth-2.21.0-py2.py3-none-any.whl (182 kB)\n  Downloading google_auth-2.20.0-py2.py3-none-any.whl (181 kB)\nINFO: pip is looking at multiple versions of google-auth to determine which version is compatible with other requirements. This could take a while.\n  Downloading google_auth-2.19.1-py2.py3-none-any.whl (181 kB)\n  Downloading google_auth-2.19.0-py2.py3-none-any.whl (181 kB)\n  Downloading google_auth-2.18.1-py2.py3-none-any.whl (178 kB)\n  Downloading google_auth-2.18.0-py2.py3-none-any.whl (178 kB)\n  Downloading google_auth-2.17.3-py2.py3-none-any.whl (178 kB)\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n  Downloading google_auth-2.17.2-py2.py3-none-any.whl (178 kB)\n  Downloading google_auth-2.17.1-py2.py3-none-any.whl (178 kB)\n  Downloading google_auth-2.17.0-py2.py3-none-any.whl (178 kB)\n  Downloading google_auth-2.16.3-py2.py3-none-any.whl (177 kB)\n  Downloading google_auth-2.16.2-py2.py3-none-any.whl (177 kB)\n  Downloading google_auth-2.16.1-py2.py3-none-any.whl (177 kB)\n  Downloading google_auth-2.16.0-py2.py3-none-any.whl (177 kB)\n  Downloading google_auth-2.15.0-py2.py3-none-any.whl (177 kB)\n  Downloading google_auth-2.14.1-py2.py3-none-any.whl (175 kB)\nINFO: pip is looking at multiple versions of google-api-core to determine which version is compatible with other requirements. This could take a while.\nINFO: pip is looking at multiple versions of google-api-core[grpc] to determine which version is compatible with other requirements. This could take a while.\nCollecting google-api-core<3.0dev,>=1.29.0\n  Downloading google_api_core-2.11.1-py3-none-any.whl (120 kB)\n  Downloading google_api_core-2.11.0-py3-none-any.whl (120 kB)\n  Downloading google_api_core-2.10.2-py3-none-any.whl (115 kB)\nCollecting google-auth<3.0dev,>=1.25.0\n  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\nRequirement already satisfied: setuptools>=40.3.0 in /databricks/python3/lib/python3.9/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (61.2.0)\nCollecting google-crc32c<2.0dev,>=1.0\n  Downloading google_crc32c-1.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\nCollecting grpcio-status<2.0dev,>=1.33.2\n  Downloading grpcio_status-1.59.0-py3-none-any.whl (14 kB)\n  Downloading grpcio_status-1.58.0-py3-none-any.whl (14 kB)\n  Downloading grpcio_status-1.57.0-py3-none-any.whl (5.1 kB)\n  Downloading grpcio_status-1.56.2-py3-none-any.whl (5.1 kB)\n  Downloading grpcio_status-1.56.0-py3-none-any.whl (5.1 kB)\n  Downloading grpcio_status-1.55.3-py3-none-any.whl (5.1 kB)\n  Downloading grpcio_status-1.54.3-py3-none-any.whl (5.1 kB)\n  Downloading grpcio_status-1.54.2-py3-none-any.whl (5.1 kB)\n  Downloading grpcio_status-1.54.0-py3-none-any.whl (5.1 kB)\n  Downloading grpcio_status-1.53.2-py3-none-any.whl (5.1 kB)\n  Downloading grpcio_status-1.53.1-py3-none-any.whl (5.1 kB)\n  Downloading grpcio_status-1.53.0-py3-none-any.whl (5.1 kB)\n  Downloading grpcio_status-1.51.3-py3-none-any.whl (5.1 kB)\n  Downloading grpcio_status-1.51.1-py3-none-any.whl (5.1 kB)\n  Downloading grpcio_status-1.50.0-py3-none-any.whl (14 kB)\n  Downloading grpcio_status-1.49.1-py3-none-any.whl (14 kB)\n  Downloading grpcio_status-1.48.2-py3-none-any.whl (14 kB)\nCollecting pyasn1<0.6.0,>=0.4.6\n  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.3)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2021.10.8)\nInstalling collected packages: pyasn1, rsa, pyasn1-modules, protobuf, cachetools, grpcio, googleapis-common-protos, google-auth, grpcio-status, google-crc32c, google-api-core, proto-plus, google-resumable-media, google-cloud-core, google-cloud-texttospeech, google-cloud-storage, google-cloud-speech, google-cloud-bigquery, google-cloud-automl\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.19.4\n    Not uninstalling protobuf at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-6261f2db-f7e6-4e69-9616-e6e862c5f3e1\n    Can't uninstall 'protobuf'. No files were found to uninstall.\nSuccessfully installed cachetools-4.2.4 google-api-core-1.34.0 google-auth-1.35.0 google-cloud-automl-2.11.3 google-cloud-bigquery-2.3.1 google-cloud-core-1.7.3 google-cloud-speech-2.22.0 google-cloud-storage-2.1.0 google-cloud-texttospeech-2.14.2 google-crc32c-1.5.0 google-resumable-media-1.3.3 googleapis-common-protos-1.61.0 grpcio-1.59.2 grpcio-status-1.48.2 proto-plus-1.22.3 protobuf-3.20.3 pyasn1-0.5.0 pyasn1-modules-0.3.0 rsa-4.9\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pip install google-cloud-automl google-cloud-storage google-cloud-bigquery==2.3.1 google-cloud-texttospeech google-cloud-speech\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0efad3a-c645-44a8-8f5a-7782465d9e34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import library\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.exceptions import NotFound\n",
    "import google.cloud.bigquery\n",
    "from google.oauth2 import service_account\n",
    "import functools\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f2fad47-dd0e-4e7c-9613-63ab11f6a0c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# GCP project variable\n",
    "\n",
    "# Project Name\n",
    "project_id = 'sakshimukkirwar-hw3-proj'\n",
    "\n",
    "# BigQuery dataset\n",
    "project_dataset = 'dataset_1'\n",
    "\n",
    "# BigQuery table name\n",
    "BQ_table = 'covid_19_geographic_receive'\n",
    "\n",
    "# GCP Authentication file\n",
    "BQ_Credentials = '/FileStore/tables/sakshimukkirwar_hw3_proj_17cc082fd25d.json'\n",
    "\n",
    "# Copy authentication file into Databrick environment at temp location\n",
    "src_path = BQ_Credentials  # This takes the path from the variable you set earlier\n",
    "dst_path = \"file:///tmp/sakshmukkirwar_hw3_proj_17cc082fd25d.json\"  # Destination on the local file system\n",
    "dbutils.fs.cp(src_path, dst_path)\n",
    "\n",
    "#Set local variables for credentials\n",
    "service_account_json = \"/tmp/sakshmukkirwar_hw3_proj_17cc082fd25d.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51250ff5-233e-4825-9373-0914921b9ec6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1942500154299085>:3\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession\u001B[38;5;66;03m# Configure Spark to use the Google Cloud Auth\u001B[39;00m\n",
       "\u001B[1;32m      2\u001B[0m spark\u001B[38;5;241m.\u001B[39mconf\u001B[38;5;241m.\u001B[39mset(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgoogle.cloud.auth.service.account.enable\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m----> 3\u001B[0m spark\u001B[38;5;241m.\u001B[39mconf\u001B[38;5;241m.\u001B[39mset(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgoogle.cloud.auth.service.account.json.keyfile\u001B[39m\u001B[38;5;124m\"\u001B[39m, service_account_json)\n",
       "\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m      6\u001B[0m     \u001B[38;5;66;03m# Read data from BigQuery using Spark\u001B[39;00m\n",
       "\u001B[1;32m      7\u001B[0m     df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbigquery\u001B[39m\u001B[38;5;124m'\u001B[39m) \\\n",
       "\u001B[1;32m      8\u001B[0m         \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcredentialsFile\u001B[39m\u001B[38;5;124m\"\u001B[39m, service_account_json) \\\n",
       "\u001B[1;32m      9\u001B[0m         \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparentProject\u001B[39m\u001B[38;5;124m\"\u001B[39m, project_id) \\\n",
       "\u001B[1;32m     10\u001B[0m         \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproject\u001B[39m\u001B[38;5;124m\"\u001B[39m, project_id) \\\n",
       "\u001B[1;32m     11\u001B[0m         \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, project_dataset) \\\n",
       "\u001B[1;32m     12\u001B[0m         \u001B[38;5;241m.\u001B[39mload(BQ_table)\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'service_account_json' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-1942500154299085>:3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession\u001B[38;5;66;03m# Configure Spark to use the Google Cloud Auth\u001B[39;00m\n\u001B[1;32m      2\u001B[0m spark\u001B[38;5;241m.\u001B[39mconf\u001B[38;5;241m.\u001B[39mset(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgoogle.cloud.auth.service.account.enable\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 3\u001B[0m spark\u001B[38;5;241m.\u001B[39mconf\u001B[38;5;241m.\u001B[39mset(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgoogle.cloud.auth.service.account.json.keyfile\u001B[39m\u001B[38;5;124m\"\u001B[39m, service_account_json)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;66;03m# Read data from BigQuery using Spark\u001B[39;00m\n\u001B[1;32m      7\u001B[0m     df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbigquery\u001B[39m\u001B[38;5;124m'\u001B[39m) \\\n\u001B[1;32m      8\u001B[0m         \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcredentialsFile\u001B[39m\u001B[38;5;124m\"\u001B[39m, service_account_json) \\\n\u001B[1;32m      9\u001B[0m         \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparentProject\u001B[39m\u001B[38;5;124m\"\u001B[39m, project_id) \\\n\u001B[1;32m     10\u001B[0m         \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproject\u001B[39m\u001B[38;5;124m\"\u001B[39m, project_id) \\\n\u001B[1;32m     11\u001B[0m         \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, project_dataset) \\\n\u001B[1;32m     12\u001B[0m         \u001B[38;5;241m.\u001B[39mload(BQ_table)\n\n\u001B[0;31mNameError\u001B[0m: name 'service_account_json' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'service_account_json' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Configure Spark to use the Google Cloud Auth\n",
    "spark.conf.set(\"google.cloud.auth.service.account.enable\", \"true\")\n",
    "spark.conf.set(\"google.cloud.auth.service.account.json.keyfile\", service_account_json)\n",
    "\n",
    "try:\n",
    "    # Read data from BigQuery using Spark\n",
    "    df = spark.read.format('bigquery') \\\n",
    "        .option(\"credentialsFile\", service_account_json) \\\n",
    "        .option(\"parentProject\", project_id) \\\n",
    "        .option(\"project\", project_id) \\\n",
    "        .option(\"dataset\", project_dataset) \\\n",
    "        .load(BQ_table)\n",
    "    print('Data loaded successfully')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Display the DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e214c663-c571-45ba-8128-575e48c4ca5d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#PySpark Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daead3cd-92cc-4504-957e-48cb748fb8b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+---------------------+------------------+------------------+------------------+-------------------------+------+----------------------+--------------------+\n|summary|               day|             month|              year|daily_confirmed_cases|      daily_deaths|   confirmed_cases|            deaths|countries_and_territories|geo_id|country_territory_code|       pop_data_2019|\n+-------+------------------+------------------+------------------+---------------------+------------------+------------------+------------------+-------------------------+------+----------------------+--------------------+\n|  count|             61900|             61900|             61900|                61900|             61900|             61900|             61900|                    61900| 61900|                 61777|               61777|\n|   mean|15.628933764135702| 7.067156704361874|2019.9989176090469|    1155.147237479806| 26.05546042003231|100583.07930533118|3101.6767851373183|                     null|  null|                  null|4.0987698230538875E7|\n| stddev| 8.841582027534178|2.9547764247491717|0.0328821661469521|     6779.22447876715|131.22705508460166| 607437.3814087728|15538.122301000762|                     null|  null|                  null|1.5312937934497508E8|\n|    min|                 1|                 1|              2019|                -8261|             -1918|                 0|                 0|              Afghanistan|    AD|                   ABW|                 815|\n|    max|                31|                12|              2020|               234633|              4928|          16256754|            299177|                 Zimbabwe|    ZW|                   ZWE|          1433783692|\n+-------+------------------+------------------+------------------+---------------------+------------------+------------------+------------------+-------------------------+------+----------------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#1. Summary Statistics:\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2ce3e84-ae11-4217-a243-dd3b175b8445",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+---------------------+\n|countries_and_territories|Total Confirmed Cases|\n+-------------------------+---------------------+\n|     United_States_of_...|             16256754|\n|                    India|              9884100|\n|                   Brazil|              6901952|\n|                   Russia|              2653928|\n|                   France|              2376852|\n|           United_Kingdom|              1849403|\n|                    Italy|              1843712|\n|                    Spain|              1730575|\n|                Argentina|              1498160|\n|                 Colombia|              1425774|\n|                  Germany|              1337078|\n|                   Mexico|              1250044|\n|                   Poland|              1135676|\n|                     Iran|              1108269|\n|                   Turkey|               995471|\n|                     Peru|               984973|\n|                  Ukraine|               900666|\n|             South_Africa|               860964|\n|                Indonesia|               617820|\n|              Netherlands|               612746|\n+-------------------------+---------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#2. Number of cases by country:\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df.groupBy(\"countries_and_territories\").agg(\n",
    "    F.sum(\"daily_confirmed_cases\").alias(\"Total Confirmed Cases\")\n",
    ").orderBy(F.desc(\"Total Confirmed Cases\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f90393bb-7920-4c58-8ade-5a3a2e9c34e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+---------------------+\n|countries_and_territories|Total Confirmed Cases|\n+-------------------------+---------------------+\n|                   Russia|              2653928|\n|                   Sweden|               320098|\n|              Philippines|               449400|\n|             South_Africa|               860964|\n|              Puerto_Rico|               107158|\n|                   Turkey|               995471|\n|                     Iraq|               574634|\n|                  Germany|              1337078|\n|                   Jordan|               259614|\n|                   France|              2376852|\n|                   Greece|               124534|\n|                 Slovakia|               132984|\n|                Argentina|              1498160|\n|                  Belgium|               608001|\n|                  Ecuador|               202110|\n|                    Qatar|               140961|\n|                  Myanmar|               108342|\n|                     Peru|               984973|\n|                    India|              9884100|\n|                  Belarus|               160295|\n+-------------------------+---------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#3. Filtering data: Let's say we want to filter out countries with more than 100,000 confirmed cases.\n",
    "filtered_data = df.groupBy(\"countries_and_territories\").agg(\n",
    "    F.sum(\"daily_confirmed_cases\").alias(\"Total Confirmed Cases\")\n",
    ").filter(\"`Total Confirmed Cases` > 100000\")\n",
    "\n",
    "filtered_data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb88e5a2-3863-4fcd-9e1b-d73ddeb1ca57",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+---------------------+------------+\n|countries_and_territories|Total Confirmed Cases|Total Deaths|\n+-------------------------+---------------------+------------+\n|     United_States_of_...|             16256754|      299177|\n|                    India|              9884100|      143355|\n|                   Brazil|              6901952|      181402|\n|                   Russia|              2653928|       46941|\n|                   France|              2376852|       57911|\n|           United_Kingdom|              1849403|       64170|\n|                    Italy|              1843712|       64520|\n|                    Spain|              1730575|       47624|\n|                Argentina|              1498160|       40766|\n|                 Colombia|              1425774|       39053|\n|                  Germany|              1337078|       21975|\n|                   Mexico|              1250044|      113953|\n|                   Poland|              1135676|       22864|\n|                     Iran|              1108269|       52196|\n|                   Turkey|               995471|       16199|\n|                     Peru|               984973|       36677|\n|                  Ukraine|               900666|       15247|\n|             South_Africa|               860964|       23276|\n|                Indonesia|               617820|       18819|\n|              Netherlands|               612746|       10034|\n+-------------------------+---------------------+------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#4. Deaths vs Confirmed Cases by country:\n",
    "df.groupBy(\"countries_and_territories\").agg(\n",
    "    F.sum(\"daily_confirmed_cases\").alias(\"Total Confirmed Cases\"),\n",
    "    F.sum(\"daily_deaths\").alias(\"Total Deaths\")\n",
    ").orderBy(F.desc(\"Total Confirmed Cases\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f073a225-5530-4e42-a6c1-c06faf581691",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+---------------------+------------+------------------+\n|countries_and_territories|Total Confirmed Cases|Total Deaths|Mortality Rate (%)|\n+-------------------------+---------------------+------------+------------------+\n|                    Yemen|                 2083|         606|29.092654824771962|\n|                   Mexico|              1250044|      113953| 9.115919119646989|\n|               Montserrat|                   13|           1|7.6923076923076925|\n|                  Ecuador|               202110|       13875|6.8650734748404325|\n|              Isle_of_Man|                  370|          25| 6.756756756756757|\n|                    Sudan|                21386|        1347| 6.298513045917891|\n|                  Bolivia|               147150|        9018| 6.128440366972477|\n|                     Chad|                 1770|         102| 5.762711864406779|\n|                    Egypt|               121575|        6920| 5.691959695661115|\n|                    Syria|                 9166|         518| 5.651320096006982|\n|                    China|                92021|        4739|   5.1499114332598|\n|                  Liberia|                 1676|          83| 4.952267303102626|\n|                     Iran|              1108269|       52196| 4.709686908142338|\n|                 Guernsey|                  289|          13| 4.498269896193772|\n|                     Fiji|                   46|           2|4.3478260869565215|\n|     United_Republic_o...|                  509|          21|  4.12573673870334|\n|              Afghanistan|                49273|        1971|  4.00016236072494|\n|                     Peru|               984973|       36677|3.7236553692334717|\n|                    Niger|                 2258|          80|  3.54295837023915|\n|                    Italy|              1843712|       64520|3.4994619550124964|\n+-------------------------+---------------------+------------+------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#5 Mortality Rate by Country: To determine the mortality rate, we'll calculate the ratio of the total number of deaths to the total number of confirmed cases for each country and then multiply by 100 to get the percentage.\n",
    "\n",
    "mortality_rate_df = df.groupBy(\"countries_and_territories\").agg(\n",
    "    F.sum(\"daily_confirmed_cases\").alias(\"Total Confirmed Cases\"),\n",
    "    F.sum(\"daily_deaths\").alias(\"Total Deaths\")\n",
    ").withColumn(\n",
    "    \"Mortality Rate (%)\",\n",
    "    (F.col(\"Total Deaths\") / F.col(\"Total Confirmed Cases\") * 100)\n",
    ").orderBy(F.desc(\"Mortality Rate (%)\"))\n",
    "\n",
    "mortality_rate_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e512542b-d97e-4dbd-8eea-d8b3cb5f4f28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configure Spark to use the Google Cloud Auth\n",
    "spark.conf.set(\"google.cloud.auth.service.enable\", \"true\")\n",
    "spark.conf.set(\"google.cloud.auth.service.account.json.keyfile\", \"/FileStore/tables/sakshimukkirwar_hw3_proj_17cc082fd25d.json\")\n",
    "\n",
    "# Create a DataFrame (replace this with your actual data loading logic)\n",
    "df = spark.read.format('bigquery') \\\n",
    "    .option(\"table\", \"sakshimukkirwar-hw3-proj.dataset_1.covid19_destination_table\") \\\n",
    "    .load()\n",
    "\n",
    "destination_table = \"sakshimukkirwar-hw3-proj.dataset_1.covid19_destination_table\"\n",
    "\n",
    "# Write the DataFrame to BigQuery\n",
    "df.write.format(\"bigquery\") \\\n",
    "    .option(\"table\", destination_table) \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "smukkirwar_hw3_q4",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
