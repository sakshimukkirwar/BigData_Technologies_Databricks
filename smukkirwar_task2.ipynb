{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3b9981b-bed0-4d37-93c0-57accf862a89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06e8d725-8fd6-41d4-9212-66b3b96980a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f236944-0b01-4943-b404-de1d9ff84380",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "The spark context has stopped and the driver is restarting. Your notebook will be automatically reattached.",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "from typing import Iterable, List\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Set the number of dimensions to 32 for the Breast Cancer dataset features\n",
    "D = 32\n",
    "\n",
    "def readPointBatch(iterator: Iterable[str]) -> List[np.ndarray]:\n",
    "    strs = list(iterator)\n",
    "    matrix = np.zeros((len(strs), D + 1))  # Adjusted for D features + 1 label\n",
    "    for i, s in enumerate(strs):\n",
    "        # Assuming the CSV has no header and the label is in the first column\n",
    "        matrix[i] = np.fromstring(s, dtype=np.float32, sep=',')\n",
    "    return matrix\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 3:\n",
    "        print(\"Usage: logistic_regression <file> <iterations>\", file=sys.stderr)\n",
    "        sys.exit(-1)\n",
    "\n",
    "    spark = SparkSession.builder.appName(\"PythonLR\").getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "points = sc.textFile('/FileStore/tables/breastcancer_data.csv').filter(lambda line: isinstance(line, str) and not line.startswith('id')).mapPartitions(readPointBatch).cache()\n",
    "\n",
    "iterations = 100 \n",
    "\n",
    "    # Initialize weights w to a random value\n",
    "w = 2 * np.random.rand(1,D) - 1\n",
    "\n",
    "print(\"Initial w: \" + str(w))\n",
    "\n",
    "def gradient(matrix: np.ndarray, w: np.ndarray) -> np.ndarray:\n",
    "    print('matrix shape', matrix.shape())\n",
    "\n",
    "    Y = matrix[:, 0]  # Extract labels\n",
    "    X = matrix[:, 1:]  # Extract features\n",
    "\n",
    "    print(\"Shape of the matrix:\", matrix.shape)\n",
    "    print(\"Shape of Y:\", Y.shape)\n",
    "    print(\"Shape of X:\", X.shape)\n",
    "    # Compute the gradient\n",
    "    grad = ((1.0 / (1.0 + np.exp(-Y * X.dot(w))) - 1.0) * Y[:, None] * X).sum(0)\n",
    "\n",
    "    return grad \n",
    "    \n",
    "def add(x: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "    x += y\n",
    "\n",
    "    return x\n",
    "\n",
    "for i in range(iterations):\n",
    "    print(f\"On iteration {i + 1}\")\n",
    "    # Initialize weights w to a random value\n",
    "    w-= points.map(lambda m: gradient (m, w) ).reduce(add)\n",
    "    print(\"Final w: \" + str(w))\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a75fe8f-469b-4dbc-a9e6-a0ba5fddb54b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ee0f7ff-5057-4149-9a44-19ba47b31ecc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder.appName(\"LogisticRegressionBreastCancer\").getOrCreate()\n",
    "\n",
    "# Read the breast cancer dataset\n",
    "data = spark.read.csv(\"/FileStore/tables/breast_cancer.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Select relevant features and target column\n",
    "selected_data = data.select(\"diagnosis\", \"radius_mean\", \"texture_mean\", \"perimeter_mean\", \"area_mean\", \n",
    "                            \"smoothness_mean\", \"compactness_mean\", \"concavity_mean\", \"concave points_mean\", \n",
    "                            \"symmetry_mean\", \"fractal_dimension_mean\", \"radius_se\", \"texture_se\", \"perimeter_se\", \n",
    "                            \"area_se\", \"smoothness_se\", \"compactness_se\", \"concavity_se\", \"concave points_se\", \n",
    "                            \"symmetry_se\", \"fractal_dimension_se\", \"radius_worst\", \"texture_worst\", \n",
    "                            \"perimeter_worst\", \"area_worst\", \"smoothness_worst\", \"compactness_worst\", \n",
    "                            \"concavity_worst\", \"concave points_worst\", \"symmetry_worst\", \"fractal_dimension_worst\")\n",
    "\n",
    "# Drop rows with missing values\n",
    "selected_data = selected_data.na.drop()\n",
    "\n",
    "# Assemble features into a vector\n",
    "feature_cols = [\"radius_mean\", \"texture_mean\", \"perimeter_mean\", \"area_mean\", \n",
    "                \"smoothness_mean\", \"compactness_mean\", \"concavity_mean\", \"concave points_mean\", \n",
    "                \"symmetry_mean\", \"fractal_dimension_mean\", \"radius_se\", \"texture_se\", \"perimeter_se\", \n",
    "                \"area_se\", \"smoothness_se\", \"compactness_se\", \"concavity_se\", \"concave points_se\", \n",
    "                \"symmetry_se\", \"fractal_dimension_se\", \"radius_worst\", \"texture_worst\", \n",
    "                \"perimeter_worst\", \"area_worst\", \"smoothness_worst\", \"compactness_worst\", \n",
    "                \"concavity_worst\", \"concave points_worst\", \"symmetry_worst\", \"fractal_dimension_worst\"]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "assembled_data = assembler.transform(selected_data)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data = assembled_data.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"diagnosis\")\n",
    "\n",
    "# Fit the model to the training data\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "# Make predictions on both the train and test data\n",
    "train_predictions = lr_model.transform(train_data)\n",
    "test_predictions = lr_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model using a BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"diagnosis\")\n",
    "\n",
    "# Calculate the train and test accuracy\n",
    "train_accuracy = evaluator.evaluate(train_predictions)\n",
    "test_accuracy = evaluator.evaluate(test_predictions)\n",
    "\n",
    "# Print the accuracies\n",
    "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "smukkirwar_hw3_q2",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
