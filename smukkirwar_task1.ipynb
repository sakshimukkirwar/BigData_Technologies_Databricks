{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa8ce420-1a87-429b-9fec-2fe964fd0cee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "881d1f8a-45c5-4324-a1f9-22ba0756ca64",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the Simple English Wikipedia data\n",
    "wikiData = spark.read.text(\"/FileStore/tables/simplewiki_latest_pages_articles__xml.bz2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a98dd1a-4480-4536-a0f1-aa6e5aa31f04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+------------------+------------------+\n|                page|out_links|      contribution|              rank|\n+--------------------+---------+------------------+------------------+\n| ################...|        1|0.8031255956592774|0.8326567563103858|\n|         Julpe River|        1|0.8031255956592774|0.8326567563103858|\n|      Bath, Somerset|        1|0.8031255956592774|0.8326567563103858|\n| code from templates|        1|0.8031255956592774|0.8326567563103858|\n|   Cessna Citation I|        1|0.8031255956592774|0.8326567563103858|\n|        Kurt Widmann|        1|0.8031255956592774|0.8326567563103858|\n| Image:Mountain_o...|        1|0.8031255956592774|0.8326567563103858|\n| Air Force Commen...|        1|0.8031255956592774|0.8326567563103858|\n|     This templat...|        1|0.8031255956592774|0.8326567563103858|\n|       Lisa Grabner |        1|0.8031255956592774|0.8326567563103858|\n+--------------------+---------+------------------+------------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Extract links from the text function\n",
    "def extract_links(text):\n",
    "    link_pattern = r'\\[\\[(.+?)([|\\]]|$)'\n",
    "    return re.findall(link_pattern, text)\n",
    "\n",
    "# Process the data using Spark RDD\n",
    "links = wikiData.rdd.flatMap(lambda x: extract_links(x[0])) \\\n",
    "    .map(lambda x: (x[0].split('|')[0], 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .map(lambda x: (x[0], x[1]))\n",
    "\n",
    "# Convert RDD to DataFrame\n",
    "linksDF = links.toDF([\"page\", \"out_links\"])\n",
    "\n",
    "# Initialize PageRank\n",
    "initialPageRanks = linksDF.withColumn(\"rank\", col(\"out_links\") * 0.15)\n",
    "\n",
    "# Iteratively calculate PageRank\n",
    "for _ in range(10):\n",
    "    currentRanks = initialPageRanks.select(\"page\", \"rank\")\n",
    "    contributions = linksDF.join(currentRanks, \"page\") \\\n",
    "        .withColumn(\"contribution\", col(\"rank\") / col(\"out_links\")) \\\n",
    "        .groupBy(\"page\") \\\n",
    "        .sum(\"contribution\") \\\n",
    "        .withColumnRenamed(\"sum(contribution)\", \"contribution\")\n",
    "\n",
    "    newPageRanks = linksDF.join(contributions, \"page\", \"left_outer\") \\\n",
    "        .fillna(0, subset=[\"contribution\"]) \\\n",
    "        .withColumn(\"rank\", col(\"contribution\") * 0.85 + 0.15)\n",
    "    initialPageRanks = newPageRanks\n",
    "\n",
    "# Display top 10 PageRanks\n",
    "initialPageRanks.orderBy(col(\"rank\").desc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8353f78b-44ab-4c6c-8181-97da4c2c6dde",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f44294ea-8ffe-4577-a271-a019c7f99bda",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B', 0.6801633626023985), ('A', 1.2528717283018032), ('C', 1.26383931343652)]\n"
     ]
    }
   ],
   "source": [
    "# Read the Simple English Wikipedia data\n",
    "wikiData = spark.read.text(\"/FileStore/tables/simplewiki_latest_pages_articles__xml.bz2\")\n",
    "\n",
    "# Sample data: (URL, neighbors)\n",
    "links = sc.parallelize([\n",
    "    ('A', ['B', 'C']),\n",
    "    ('B', ['C']),\n",
    "    ('C', ['A']),\n",
    "    ('D', ['C'])\n",
    "]).partitionBy(4).persist()\n",
    "\n",
    "# Initialize each URL's rank to 1.0\n",
    "ranks = links.mapValues(lambda neighbors: 1.0)\n",
    "\n",
    "# Number of iterations\n",
    "for i in range(10):\n",
    "    # Calculates URL contributions to the rank of other URLs\n",
    "    contributions = links.join(ranks).flatMap(\n",
    "        lambda url_urls_rank: \n",
    "            [(dest, url_urls_rank[1][1] / len(url_urls_rank[1][0])) for dest in url_urls_rank[1][0]]\n",
    "    )\n",
    "\n",
    "    # Update the ranks based on contributions\n",
    "    ranks = contributions.reduceByKey(lambda a, b: a + b).mapValues(lambda rank: 0.15 + 0.85 * rank)\n",
    "\n",
    "# Collect and print the final ranks\n",
    "print(ranks.collect())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5239614d-a453-40b4-af00-9cd80fa2d318",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "smukkirwar_hw3_qs1",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
