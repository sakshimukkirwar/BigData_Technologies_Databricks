{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52929dad-81cf-4932-a114-e29b0fe6a82c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##Python Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d518b54-2abd-41d2-8058-75cae9b9dfa0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Bucket:\n",
    "    def __init__(self, sum_value, timestamp):\n",
    "        self.sum_value = sum_value\n",
    "        self.timestamp = timestamp\n",
    "\n",
    "class DGIM:\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.buckets = []\n",
    "        self.timestamp = 0\n",
    "\n",
    "    def update(self, value):\n",
    "        self.timestamp += 1\n",
    "        self._remove_old_buckets()\n",
    "        \n",
    "        # Add new bucket for current value\n",
    "        self.buckets.append(Bucket(value, self.timestamp))\n",
    "        \n",
    "        self._combine_buckets()\n",
    "\n",
    "    def _remove_old_buckets(self):\n",
    "        while self.buckets and self.buckets[0].timestamp <= self.timestamp - self.k:\n",
    "            self.buckets.pop(0)\n",
    "\n",
    "    def _combine_buckets(self):\n",
    "        sizes = {}\n",
    "        i = len(self.buckets) - 1\n",
    "        while i >= 0:\n",
    "            size = self.buckets[i].sum_value\n",
    "            if size not in sizes:\n",
    "                sizes[size] = 1\n",
    "            else:\n",
    "                sizes[size] += 1\n",
    "\n",
    "            if sizes[size] > 2:\n",
    "                # Combine two oldest buckets of this size\n",
    "                b1 = self.buckets[i-1]\n",
    "                b2 = self.buckets[i]\n",
    "                self.buckets.pop(i-1)\n",
    "                self.buckets.pop(i-1)\n",
    "                combined_bucket = Bucket(b1.sum_value + b2.sum_value, b2.timestamp)\n",
    "                self.buckets.insert(i-1, combined_bucket)\n",
    "                sizes[size] = 1\n",
    "                i -= 1\n",
    "            i -= 1\n",
    "\n",
    "    def get_sum(self):\n",
    "        total = 0\n",
    "        for bucket in self.buckets:\n",
    "            total += bucket.sum_value\n",
    "        return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d81f490f-574a-4af9-9cc6-af748260934a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "babdd7f4-8bca-483b-aaf4-fa58659d9962",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Spark Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7be7e333-f261-478a-922f-bd85c8a71443",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DGIM Results: [0, 1, 1, 0, 1, 0, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder.appName(\"DGIM Algorithm\").getOrCreate()\n",
    "# Initialize a Spark context\n",
    "sc = SparkContext.getOrCreate()\n",
    "def run_dgim_algorithm(input_data, k):\n",
    "    # Create an RDD from your input data\n",
    "    input_rdd = sc.parallelize(input_data)\n",
    "    # Create an instance of the DGIM class\n",
    "    dgim = DGIM(k)\n",
    "\n",
    "    # Define a function to update the DGIM instance for each input value\n",
    "    def update_dgim(value):\n",
    "        dgim.update(value)\n",
    "        return dgim.get_sum()\n",
    "\n",
    "    # Use the map transformation to update DGIM for each value in the input RDD\n",
    "    result_rdd = input_rdd.map(update_dgim)\n",
    "\n",
    "    # Collect the results into a list\n",
    "    result_list = result_rdd.collect()\n",
    "\n",
    "    # Return the final results\n",
    "    return result_list\n",
    "if __name__ == \"__main__\":\n",
    "    # Define your input data as a list of values\n",
    "    input_data = [0, 1, 1, 0, 1, 0, 0, 1, 0, 0]\n",
    "\n",
    "    # Define the parameter 'k' for DGIM\n",
    "    k = 4\n",
    "    # Run the DGIM algorithm on the input data\n",
    "    result = run_dgim_algorithm(input_data, k)\n",
    "    # Print the results\n",
    "    print(\"DGIM Results:\", result)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c81e81db-eacf-461d-8750-471a3002987e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "The spark context has stopped and the driver is restarting. Your notebook will be automatically reattached.",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "   # Stop the Spark context\n",
    "sc.stop()\n",
    "\n",
    "    # Stop the Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "smukkirwar_hw3_q3",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
